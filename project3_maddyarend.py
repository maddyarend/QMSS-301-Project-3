# -*- coding: utf-8 -*-
"""Project3_MaddyArend.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rC9SuFYEWOAjbvw6xQIg01gsTFjpwQS-
"""

# Maddy Arend
# Project 3

#import libraries for reddit scraping
!pip install praw
import requests
import pandas as pd
import praw

# pass credentials
healthcare_comments = praw.Reddit(client_id='OprG03zfb1O8bj1Fpdnspg',
                     client_secret='kU3-ROFJnc_tzH2EU0kpysuKYQ5qDg',
                     user_agent='qmss 301')

# create object with url of the post
url = "https://www.reddit.com/r/AskReddit/comments/piok3m/americans_do_you_want_universal_healthcare_whywhy/"

# create a submission object
submission = healthcare_comments.submission(url=url)

# scrape comments from the post

post_comments = [] # create an empty list

submission.comments.replace_more(limit=None) # loop through all comments, no limit to how many it scrapes
for comment in submission.comments.list():
 post_comments.append([comment.body, comment.ups, comment.downs, comment.created]) # append the comment information to the empty list

# convert the post_comments list to a dataframe
comments_df = pd.DataFrame(post_comments, columns=['comment', 'ups', 'downs', 'created'])
comments_df # print the dataframe

# save dataframe as a csv file
comments_df.to_csv('healthcare_data.csv', index=True, header=True)

# read in data
comments_df = pd.read_csv('healthcare_data.csv')
comments_df.head() # print first five rows of dataframe

# drop duplicates
comments_df.drop_duplicates(subset='comment', keep='last', inplace=True)

# determine how many comments are in the dataset
comments_df.shape

# remove unneccesary columns
comments_df = comments_df.drop(columns = ['Unnamed: 0'], axis = 1)
comments_df.head() # print first five rows of dataframe to verify columns were removed

# converting the created date to an easily readable format

from datetime import datetime # import necessary function from datetime package

comments_df['date_time'] = pd.to_datetime(comments_df['created'],  unit='s') # create new column with date and time of post creation
comments_df['date'] = comments_df['date_time'].dt.strftime('%Y-%m-%d') # create new column with only date of post creation

comments_df.head() # print first 5 rows of dataframe to check new columns are there

# create new column with character count of each comment
comments_df['character_count'] = comments_df['comment'].apply(lambda x: len(x))
comments_df # print dataframe to check character count column is there

# create new column with the word count of each comment
comments_df['word_count'] = comments_df['comment'].apply(lambda x: len(x.split()))
comments_df # print dataframe to check word count column is there

# preprocess the data

import re # import regular expression

import nltk  # import nltk library
nltk.download('stopwords') # download a set of commonly used stopwords
from nltk.corpus import stopwords # import the stopwords
STOPWORDS = set(stopwords.words('english')) # set STOPWORDS equal to the set of englishs stopwords

# create cleaning function
def reddit_clean (redd):
    redd = str(redd).lower() # convert the text in the comments to lowercase
    redd = re.sub("'", "", redd) # remove single quotes
    redd = re.sub("@[A-Za-z0-9_]+","", redd) # remove patterns that start with '@'
    redd = re.sub("#[A-Za-z0-9_]+","", redd) # remove patterns that start with '#'
    redd = re.sub(r"www.\S+", "", redd) # remove URLs that start with 'www.'
    redd = re.sub(r"http\S+", "", redd) # remove URLs
    redd = re.sub('[()!?]', ' ', redd) # replace parentheses, exclamation marks, and question marks with spaces
    redd = re.sub('\[.*?\]',' ', redd) # remove text within square brackets
    redd = re.sub("[^a-z0-9]"," ", redd) # replace non-alphanumeric characters with spaces
    redd = re.sub(r"\b\w{1,3}\b"," ", redd) # remove words with less than 4 characters.
    redd = redd.split() # split the text into a list of words
    stopwords = STOPWORDS  # assign the set of stopwords to a variable called stopwords
    redd = [w for w in redd if not w in stopwords] # remove stopwords from the list of words
    redd = " ".join(word for word in redd) # join the remaining words back into a string
    return redd  # return the cleaned text

# apply the reddit_clean function to clean the 'comment' column of the comments_df dataframe
comments_df['comment'] = comments_df['comment'].apply(reddit_clean)
comments_df.head() # display the first five rows of the cleaned dataframe

# create word cloud

# import necessary packages
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import collections

# get all words in the comments and count frequencies
all_words = ' '.join([text for text in comments_df['comment']]) # puts all the words from all the comments into a single string
count_word = collections.Counter(all_words.split()) # tokenize the all_words string and count the frequencies of each word

# create word cloud
wordcloud = WordCloud(width=800, height=500) # width and height of wordclould in pixels
wordcloud.generate_from_frequencies(count_word) #generate word cloud based on word frequencies
plt.figure(figsize=(10, 7)) # dimensions of wordcloud in inches
plt.imshow(wordcloud) # display plot as wordcloud
plt.axis("off") # no axes
plt.show() # print plot

# print the top 15 most used words and their frequencies
count_word.most_common(15)

# create a bargraph of the frequencies of the top 15 most used words

# import necessary pacakges
import itertools
import collections
import seaborn as sns

a = nltk.FreqDist(count_word) # create frequency distribution of the words in dictionary format
d = pd.DataFrame({'Word': list(a.keys()),
                  'Count': list(a.values())}) # convert the frequency distribution dictionary into a dataframe

# select the top 15 most frequent words
d = d.nlargest(columns="Count", n = 15) # select the top 15
plt.figure(figsize=(16,5)) # dimensions of figure in inches
ax = sns.barplot(data=d, x= "Word", y = "Count") # assign variables to axes
ax.set(ylabel = 'Word Count') #label the y-axis
plt.show() # show the plot

# textblob sentiment analysis

# import textblob library
from textblob import TextBlob

# get polarity and subjectivity scores for each comment

# define a function that gets the polarity score of the text it is applied to
def get_polarity(text):
    analysis = TextBlob(text) # create a classifier for the comment
    return analysis.sentiment.polarity # return the polairty of the comment

# define a function that gets the subjectivity score of the text it is applied to
def get_subjectivity(text):
    analysis = TextBlob(text) # create a classifier for the comment
    return analysis.sentiment.subjectivity # return the subjectivity of the comment

comments_df['textblob polarity'] = comments_df['comment'].apply(get_polarity) # apply the polarity score function to each comment and create a new column to house the scores
comments_df['subjectivity'] = comments_df['comment'].apply(get_subjectivity) # apply the subjectivity function to each comment and create a new column to house the scores
comments_df.head() # print first 5 rows of dataframe

# classify the textblob polarity

def analyze_sentiment(text): # define a function to assess polarity
    blob = TextBlob(text) # create a classifier
    sentiment_polarity = blob.sentiment.polarity
    if sentiment_polarity > 0: # if polarity is above 0
        return 'Positive Sentiment' # return positive statement
    elif sentiment_polarity == 0: # if polarity is 0
        return 'Neutral Sentiment' # return neutral statement
    else: # otherwise
        return 'Negative Sentiment' # return negative statement

# apply polarity function to textblob polarity column
comments_df['textblob sentiment'] = comments_df['comment'].apply(analyze_sentiment)
comments_df.head() # print first 5 rows of dataframe

# classify the subjectivity of each comment

# import necessary packages
import numpy as np

comments_df['subjectivity2'] = np.where(comments_df['subjectivity'] > 0.5, 'Subjective', 'Objective') # if subjectivity score is above 0.5, return subjective, otherwise return objective
comments_df.head() # print first 5 rows of dataframe

# count the number of comments under each sentiment category
comments_df['textblob sentiment'].value_counts()

# amount of comments under each sentiment category in percentage terms
print("Positive Sentiment:", (7583/15400)*100, "%")
print("Neutral Sentiment", (3359/15400)*100, "%")
print("Negative Sentiment", (4458/15400)*100, "%")

# create a histogram of the sentiment scores
plt.figure(figsize=(10,5)) # dimensions of the histogram in inches
plt.xlabel('Sentiment', fontsize=30) # x axis label
plt.xticks(fontsize=15) # font size for x axis tick mark labels
plt.ylabel('Frequency', fontsize=30) # y axis label
plt.yticks(fontsize=15) # font size for y axis tick mark labels
plt.hist(comments_df['textblob polarity'], bins=10) # create 10 bins based on textblob polarity scores
plt.title('Textblob Sentiment Distribution', fontsize=30) # title of plot
plt.show() # show plot

# count the number of cases that fall within each subjectivity category
comments_df['subjectivity2'].value_counts()

# amount of comments under each subjectivity category in percentage terms
print("Objective:", (10203/15400)*100, "%")
print("Subjective", (5197/15400)*100, "%")

# create a histogram of the subjectivity scores
plt.figure(figsize=(10,5)) # dimensions of the histogram in inches
plt.xlabel('Subjectivity', fontsize=30) # x axis label
plt.xticks(fontsize=15) # font size for x axis tick mark labels
plt.ylabel('Frequency', fontsize=30) # y axis label
plt.yticks(fontsize=15) # font size for y axis tick mark labels
plt.hist(comments_df['subjectivity'], bins=10) # create 10 bins based on subjectivity scores
plt.title('Subjectivity Distribution', fontsize=30) # title of plot
plt.show() # show plot

# get summary statistics of polarity and subjectivity
pol_sub = comments_df[["textblob polarity", "subjectivity"]] # make a list of columns to analyze
pol_sub.describe() # get summary statistics of the variables

# create crosstabs comparing polarity and subjectivity
pd.crosstab(comments_df['textblob sentiment'], comments_df['subjectivity2'])

# vader sentiment analysis 4458

# create a new dataframe from comments_df dataframe so that modifications can be made without overwriting old dataframe
comments_df2 = comments_df

comments_df2.columns

# remove columns related to textblob analysis
comments_df2 = comments_df2.drop(columns = ['subjectivity', 'textblob polarity', 'textblob sentiment', 'subjectivity2'], axis = 1)
comments_df2.head() # print first 5 rows of dataframe to check columns have been removed

# install necessary packages
!pip install vaderSentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# get the compound sentiment score of each comment

sid = SentimentIntensityAnalyzer() # create a SentimentIntensityAnalyzer object

# create a function that returns the compound sentiment score of each comment
def get_vader_compound(text):
  return sid.polarity_scores(text)['compound']

comments_df2['compound_score'] = comments_df2['comment'].apply(get_vader_compound) # apply the function to the comment column of the dataframe
comments_df2.head() # print the first 5 rows of the dataframe

# get the positive score of each comment

# create a function that returns the positive sentiment score of each comment
def get_vader_pos(text):
  return sid.polarity_scores(text)['pos']

comments_df2['pos_score'] = comments_df['comment'].apply(get_vader_pos) # apply the function to the comment column of the dataframe
comments_df2.head() # print first 5 rows of dataframe

# get the negative score of each comment

# create a function that returns the negative sentiment score of each comment
def get_vader_pos(text):
  return sid.polarity_scores(text)['neg']

comments_df2['neg_score'] = comments_df['comment'].apply(get_vader_pos) # apply the function to the comment column of the dataframe
comments_df2.head() # print first 5 rows of dataframe

# get the neutral score of each comment

# create a function that returns the neutral sentiment score of each comment
def get_vader_pos(text):
  return sid.polarity_scores(text)['neu']

comments_df2['neu_score'] = comments_df['comment'].apply(get_vader_pos) # apply the function to the comment column of the dataframe
comments_df2.head() # print first 5 rows of dataframe

# classify the polarity using the compound score

# create a function that classifies the polarity of each comment
def analyze_sentiment(text): # define the function
    if text >= 0.05: # if the compound score is above 0.05
        return 'Positive Sentiment' # return "positive sentiment"
    elif text <= -0.05: # if the compound score is below -0.05
        return 'Negative Sentiment' # return "negative sentiment"
    else: # otherwise
        return 'Neutral Sentiment' # return "neutral statement"

# apply the function to the dataframe
comments_df2['sentiment'] = comments_df2['compound_score'].apply(analyze_sentiment)
comments_df2.head() # print first five rows of dataframe

# count the number of comments under each sentiment category
comments_df2['sentiment'].value_counts()

# amount of comments under each sentiment category in percentage terms
print("Positive Sentiment:", (7483/15400)*100, "%")
print("Neutral Sentiment", (2696/15400)*100, "%")
print("Negative Sentiment", (5221/15400)*100, "%")

# create a histogram of the sentiment scores
plt.figure(figsize=(10,5)) # dimensions of the histogram in inches
plt.xlabel('Sentiment', fontsize=30) # x axis label
plt.xticks(fontsize=15) # font size for x axis tick mark labels
plt.ylabel('Frequency', fontsize=30) # y axis label
plt.yticks(fontsize=15) # font size for y axis tick mark labels
plt.hist(comments_df2['compound_score'], bins=10) # create 10 bins based on VADER compound scores
plt.title('Vader Sentiment Distribution', fontsize=30) # title of plot
plt.show() # show plot

# create a wordcloud with only positive sentiment comments (based on VADER sentiment score) (extra point)

# filter data for only positive sentiment comments
positive_comments = comments_df2[comments_df2['sentiment']== 'Positive Sentiment']

# create the word cloud

# get all words in the positive comments and count frequencies
pos_words = ' '.join([text for text in positive_comments['comment']]) # puts all the words from all the positive comments into a single string
count_pos_word = collections.Counter(pos_words.split()) # tokenize the all_words string and count the frequencies of each word

# create word cloud
wordcloud = WordCloud(width=800, height=500) # width and height of wordclould in pixels
wordcloud.generate_from_frequencies(count_pos_word) #generate word cloud based on word frequencies
plt.figure(figsize=(10, 7)) # dimensions of wordcloud in inches
plt.imshow(wordcloud) # display plot as wordcloud
plt.axis("off") # no axes
plt.show() # print plot

# create a bargraph of the frequencies of the top 15 most used words in positive comments

# create frequency distribution of the words in dictionary format
b = nltk.FreqDist(count_pos_word)
e = pd.DataFrame({'Word': list(b.keys()),
                  'Count': list(b.values())}) # convert the frequency distribution dictionary into a dataframe

# select the top 15 most frequent words and create plot
e = e.nlargest(columns="Count", n = 15) # select the top 15
plt.figure(figsize=(16,5)) # dimensions of figure in inches
ax = sns.barplot(data=e, x= "Word", y = "Count") # assign variables to axes
ax.set(ylabel = 'Word Count') #label the y-axis
plt.show() # show the plot

# create a wordcloud with only negative sentiment comments (based on VADER sentiment score) (extra point)

# filter data for only negative sentiment comments
negative_comments = comments_df2[comments_df2['sentiment']== 'Negative Sentiment']

# create the word cloud

# get all words in the negative comments and count frequencies
neg_words = ' '.join([text for text in negative_comments['comment']]) # puts all the words from all the negative comments into a single string
count_neg_word = collections.Counter(neg_words.split()) # tokenize the all_words string and count the frequencies of each word

# create word cloud
wordcloud = WordCloud(width=800, height=500) # width and height of wordclould in pixels
wordcloud.generate_from_frequencies(count_neg_word) #generate word cloud based on word frequencies
plt.figure(figsize=(10, 7)) # dimensions of wordcloud in inches
plt.imshow(wordcloud) # display plot as wordcloud
plt.axis("off") # no axes
plt.show() # print plot

# create a bargraph of the frequencies of the top 15 most used words in negative comments

# create frequency distribution of the words in dictionary format
c = nltk.FreqDist(count_neg_word)
f = pd.DataFrame({'Word': list(c.keys()),
                  'Count': list(c.values())}) # convert the frequency distribution dictionary into a dataframe

# select the top 15 most frequent words and create plot
f = f.nlargest(columns="Count", n = 15) # select the top 15
plt.figure(figsize=(16,5)) # dimensions of figure in inches
ax = sns.barplot(data=f, x= "Word", y = "Count") # assign variables to axes
ax.set(ylabel = 'Word Count') #label the y-axis
plt.show() # show the plot

# build predictive model

# filter dataframe to only include positive and negative sentiment comments
filtered_comments = comments_df2[comments_df2['sentiment']!= 'Neutral Sentiment']
filtered_comments # print dataframe to verify neutral comments are removed

# get count of how many comments remain in the data
filtered_comments.shape

# investigate the potential predictive variables

# sort the comments by sentiment
filtered_sorted_comments = filtered_comments.sort_values('sentiment')

# look at average word count by sentiment category
avg_words_by_sentiment = filtered_sorted_comments.groupby('sentiment')['word_count'].mean() # group comments by sentiment and get mean word count for each group
print(avg_words_by_sentiment) # print average word count by sentiment

# create bar chart of average word count by sentiment
plt.bar(avg_words_by_sentiment.index, avg_words_by_sentiment) # create bar chart with the two sentiments on the x axis and the average word count on the y axis
plt.xlabel("Comment Sentiment") # label the x axis
plt.ylabel("Average Word Count") # label the y axis
plt.title("Average Word Count by Sentiment") # title the graph
plt.show() # show the plot

# look at average character count by sentiment category
avg_characters_by_sentiment = filtered_sorted_comments.groupby('sentiment')['character_count'].mean() # group comments by sentiment and get mean character count for each group
print(avg_characters_by_sentiment) # print average character count by sentiment

# create bar chart of average character count by sentiment
plt.bar(avg_characters_by_sentiment.index, avg_characters_by_sentiment) # create bar chart with the two sentiments on the x axis and the average character count on the y axis
plt.xlabel("Comment Sentiment") # label the x axis
plt.ylabel("Average Character Count") # label the y axis
plt.title("Average Character Count by Sentiment") # title the graph
plt.show() # show the plot

# look at average ups by sentiment category
avg_ups_by_sentiment = filtered_sorted_comments.groupby('sentiment')['ups'].mean() # group comments by sentiment and get mean ups for each group
print(avg_ups_by_sentiment) # print average ups by sentiment

# create bar chart of average up votes by sentiment
plt.bar(avg_ups_by_sentiment.index, avg_ups_by_sentiment) # create bar chart with the two sentiments on the x axis and the average up votes on the y axis
plt.xlabel("Comment Sentiment") # label the x axis
plt.ylabel("Average Number of  Up Votes") # label the y axis
plt.title("Average Number of Up Votes by Sentiment") # title the graph
plt.show() # show the plot

# look at average downs by sentiment category
avg_downs_by_sentiment = filtered_sorted_comments.groupby('sentiment')['downs'].mean() # group comments by sentiment and get mean downs for each group
print(avg_downs_by_sentiment) # print average downs by sentiment

# set features (x) and target variable (y)
x = filtered_comments[['ups','word_count','character_count']]
y = filtered_comments['sentiment']

# split data into train and test data

# import necessary package
from sklearn.model_selection import train_test_split

# use train_test_split function to use 80% of data to train model and 20% to test it
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, random_state = 10)

# import logistic regression function from sklearn library
from sklearn.linear_model import LogisticRegression

# create a logistic regression classifier for the model
clf = LogisticRegression()

# train the model by fitting the train data to the logistic regression classifier
clf.fit(x_train, y_train)

# print coefficients and intercept of the model
print("Coefficients:", clf.coef_) # printing coefficients
print("Intercept:", clf.intercept_) # printing intercept

# use the model to make predictions of the sentiment of comments based on test data
y_pred = clf.predict(x_test) # feed model with x_test data to get predicted y values (sentiments)

# calculate and print accuracy of model
accuracy2 = clf.score(x_test, y_test) # use .score function to get accuracy score of model when used on test data
print("Accuracy:", accuracy2) # print accuracy score calculated above

# create and print a confusion matrix using confusion_matrix function
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
cm

# create a heatmap using the confusion matrix
sns.heatmap(cm, annot=True, cmap='Blues', cbar=True) # use heatmap function from seaborn to create graphic
plt.xlabel('Predicted') # label x axis
plt.ylabel('Actual') # label y axis
plt.show() # show plot

# get an overall report of the accuracy of the model's predictions
from sklearn.metrics import classification_report # import necessary function from sklearn
report = classification_report(y_test, y_pred) # use classification_report function to create a report of the four model prediction accuracy scores
print(report) # print the report

"""Interpretation:

Accuracy: My model predicts 59.76% of the sentiments of the comments in the test data correctly.

Precision: Out of all of the predicted positive sentiment comments that the model made, only 59% of them were true positive sentiment comments.

Recall: The model predicted 100% of the actual positive sentiment comments in the test data.

F1-score: The harmonic mean of my model's precision and recall scores is 74%.
"""